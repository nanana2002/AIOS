import os
import json
import asyncio
from typing import List, Dict, Any

from dotenv import load_dotenv
from fastmcp import Client
from openai import OpenAI


class SyncMCPClient:
    """åŒæ­¥ MCP å®¢æˆ·ç«¯ - åŸºäº FastMCP å®˜æ–¹å®¢æˆ·ç«¯"""

    def __init__(self, base_url: str = None):
        if base_url is None:
            base_url = os.getenv('MCP_BASE_URL', 'http://127.0.0.1:9000/mcp/')
        self.base_url = base_url
        print(f"ğŸ”— FastMCP æœåŠ¡å™¨åœ°å€: {self.base_url}")

    def _run_async(self, coro):
        """è¿è¡Œå¼‚æ­¥å‡½æ•°çš„è¾…åŠ©æ–¹æ³•"""
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        return loop.run_until_complete(coro)

    async def _list_tools_async(self) -> List[Dict[str, Any]]:
        """å¼‚æ­¥è·å–å·¥å…·åˆ—è¡¨"""
        mcp_client = Client(self.base_url)

        async with mcp_client:
            tools = await mcp_client.list_tools()

            # å°†å·¥å…·å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
            tool_list = []
            for tool in tools:
                tool_dict = {
                    'name': getattr(tool, 'name', None),
                    'description': getattr(tool, 'description', ''),
                    'inputSchema': getattr(tool, 'inputSchema', {})
                }
                tool_list.append(tool_dict)

            return tool_list

    async def _call_tool_async(self, tool_name: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """å¼‚æ­¥è°ƒç”¨å·¥å…·"""
        mcp_client = Client(self.base_url)

        async with mcp_client:
            try:
                tool_result_obj = await mcp_client.call_tool(tool_name, args)
                return tool_result_obj.data if tool_result_obj else {}
            except Exception as e:
                return {"error": str(e)}

    def list_tools(self) -> List[Dict[str, Any]]:
        """åŒæ­¥è·å–å·¥å…·åˆ—è¡¨"""
        return self._run_async(self._list_tools_async())

    def call_tool(self, tool_name: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """åŒæ­¥è°ƒç”¨å·¥å…·"""
        return self._run_async(self._call_tool_async(tool_name, args))

    def close(self):
        """å…³é—­ä¼šè¯ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        pass


def format_tools_for_llm(tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """å°† MCP å·¥å…·æ ¼å¼åŒ–ä¸º LLM å¯ç”¨çš„å·¥å…·å®šä¹‰"""
    tool_defs = []

    for tool in tools:
        tool_name = tool.get('name')
        if not tool_name:
            continue

        tool_description = tool.get('description', "")
        if not isinstance(tool_description, str):
            tool_description = str(tool_description)

        tool_parameters = tool.get('inputSchema')
        if not tool_parameters:
            tool_parameters = {"type": "object", "properties": {}}
        elif isinstance(tool_parameters, str):
            try:
                tool_parameters = json.loads(tool_parameters)
            except json.JSONDecodeError:
                continue
        elif not isinstance(tool_parameters, dict):
            continue

        # ç¡®ä¿ tool_parameters åŒ…å« 'type' å’Œ 'properties' é”®
        tool_parameters.setdefault("type", "object")
        tool_parameters.setdefault("properties", {})

        tool_defs.append({
            "type": "function",
            "function": {
                "name": tool_name,
                "description": tool_description,
                "parameters": tool_parameters,
            }
        })

    return tool_defs


def chat_with_tools(user_query: str, mcp_client: SyncMCPClient,
                    tool_defs: List[Dict[str, Any]],
                    messages: List[Dict[str, Any]],
                    planner_llm: OpenAI,
                    model_name: str) -> str:
    """å¤„ç†å•æ¬¡å¯¹è¯ï¼Œå¯èƒ½æ¶‰åŠå·¥å…·è°ƒç”¨"""

    messages.append({"role": "user", "content": user_query})

    try:
        if tool_defs:
            plan_resp = planner_llm.chat.completions.create(
                model=model_name,
                messages=messages,
                tools=tool_defs,
                tool_choice="auto"
            )
        else:
            plan_resp = planner_llm.chat.completions.create(
                model=model_name,
                messages=messages
            )
    except Exception as e:
        messages.pop()  # ç§»é™¤æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        return f"é”™è¯¯ï¼šLLM è°ƒç”¨å¤±è´¥: {e}"

    llm_message = plan_resp.choices[0].message
    messages.append(llm_message)

    if llm_message.tool_calls:
        print(f"\nğŸ”§ LLM å†³å®šè°ƒç”¨ {len(llm_message.tool_calls)} ä¸ªå·¥å…·")

        # LLM å†³å®šè°ƒç”¨å·¥å…·
        for func_call in llm_message.tool_calls:
            tool_name = func_call.function.name
            try:
                args = json.loads(func_call.function.arguments)
                print(f"  - è°ƒç”¨å·¥å…·: {tool_name}")
                print(f"  - å‚æ•°: {args}")

                tool_result = mcp_client.call_tool(tool_name, args)
                print(f"  - ç»“æœ: {tool_result}")

                # å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
                messages.append({
                    "role": "tool",
                    "tool_call_id": func_call.id,
                    "name": tool_name,
                    "content": json.dumps(tool_result, ensure_ascii=False)
                })

            except json.JSONDecodeError as e:
                messages.pop()  # ç§»é™¤ç”¨æˆ·æ¶ˆæ¯
                messages.pop()  # ç§»é™¤ LLM çš„å·¥å…·è°ƒç”¨
                return f"é”™è¯¯ï¼šè§£æå·¥å…·å‚æ•°å¤±è´¥: {e}"
            except Exception as e:
                messages.pop()  # ç§»é™¤ç”¨æˆ·æ¶ˆæ¯
                messages.pop()  # ç§»é™¤ LLM çš„å·¥å…·è°ƒç”¨
                return f"é”™è¯¯ï¼šå·¥å…· {tool_name} è°ƒç”¨å¤±è´¥: {e}"

        # å†æ¬¡è°ƒç”¨ LLMï¼Œè®©å®ƒæ ¹æ®å·¥å…·ç»“æœç»™å‡ºæœ€ç»ˆå›å¤
        try:
            final_resp = planner_llm.chat.completions.create(
                model=model_name,
                messages=messages
            )

            final_content = final_resp.choices[0].message.content
            messages.append({"role": "assistant", "content": final_content})
            return final_content
        except Exception as e:
            return f"é”™è¯¯ï¼šç”Ÿæˆæœ€ç»ˆå›å¤å¤±è´¥: {e}"
    else:
        # LLM å†³å®šä¸è°ƒç”¨å·¥å…·ï¼Œç›´æ¥å›å¤
        return llm_message.content


def run_chat(user_query: str, planner_llm: OpenAI, model_name: str) -> Dict[str, Any]:
    """è¿è¡Œå•æ¬¡å¯¹è¯"""

    # MCP å®¢æˆ·ç«¯è¿æ¥
    mcp_client = SyncMCPClient()

    try:
        # è·å–å·¥å…·åˆ—è¡¨
        print("ğŸ“¡ æ­£åœ¨è·å–å·¥å…·åˆ—è¡¨...")
        tools = mcp_client.list_tools()
        print(f"ğŸ“‹ å‘ç° {len(tools)} ä¸ªå·¥å…·")

        if tools:
            for tool in tools:
                print(f"  - {tool.get('name', 'æœªçŸ¥')}: {tool.get('description', 'æ— æè¿°')}")
        else:
            print("âš ï¸  æœªå‘ç°ä»»ä½•å·¥å…·ï¼Œå°†è¿›è¡Œæ™®é€šå¯¹è¯")

        # æ„é€  LLM å¯ç”¨çš„ tool_def åˆ—è¡¨
        tool_defs = format_tools_for_llm(tools)
        print(f"ğŸ”§ æ ¼å¼åŒ– {len(tool_defs)} ä¸ªå·¥å…·å®šä¹‰")

        # å¯¹è¯å†å²ï¼ŒåŒ…æ‹¬ç³»ç»Ÿæ¶ˆæ¯
        messages = [{
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ agentï¼Œå¯ä»¥å†³å®šæ˜¯å¦è°ƒç”¨ç›¸å…³å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚ä½ ä¼šæ ¹æ®ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œå›å¤æˆ–å·¥å…·è°ƒç”¨ã€‚"
        }]

        # æ‰§è¡Œå•æ¬¡ä»»åŠ¡
        print(f"\nğŸ’­ å¤„ç†ç”¨æˆ·æŸ¥è¯¢: {user_query}")
        response = chat_with_tools(user_query, mcp_client, tool_defs, messages, planner_llm, model_name)

        return {
            "query": user_query,
            "response": response,
            "tools_available": len(tool_defs),
            "success": True
        }

    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return {
            "query": user_query,
            "error": str(e),
            "success": False
        }
    finally:
        mcp_client.close()


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•ç”¨"""
    print("ğŸš€ å¯åŠ¨ FastMCP å®¢æˆ·ç«¯æµ‹è¯•")

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv('.env')

    # é…ç½®ç¯å¢ƒå˜é‡
    os.environ['OPENAI_API_KEY'] = os.getenv('LLM_API_KEY')

    # è®¾ç½®ä»£ç†é…ç½®
    no_proxy = os.getenv('NO_PROXY', 'localhost,127.0.0.1')
    os.environ['NO_PROXY'] = no_proxy

    # è·å–æ¨¡å‹åç§°
    model_name = os.getenv('OLLAMA_MODEL_NAME', 'gpt-4o')
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")

    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    try:
        planner_llm = OpenAI()
        print("âœ… OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # æµ‹è¯•å•æ¬¡æŸ¥è¯¢
    test_queries = [
        "å½“å‰Ai Agentæœ‰å“ªäº›çŸ¥åçš„å¼€æºæ¡†æ¶"
    ]

    for query in test_queries:
        print(f"\n{'=' * 50}")
        result = run_chat(query, planner_llm, model_name)

        if result["success"]:
            print(f"âœ… æŸ¥è¯¢æˆåŠŸ")
            print(f"ğŸ” æŸ¥è¯¢: {result['query']}")
            print(f"ğŸ’¬ å›å¤: {result['response']}")
            print(f"ğŸ”§ å¯ç”¨å·¥å…·æ•°: {result['tools_available']}")
        else:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥")
            print(f"ğŸ” æŸ¥è¯¢: {result['query']}")
            print(f"âš ï¸  é”™è¯¯: {result['error']}")

        print("-" * 50)

    # äº¤äº’å¼æµ‹è¯•
    print(f"\n{'=' * 50}")
    print("ğŸ¯ è¿›å…¥äº¤äº’æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
    print("=" * 50)

    while True:
        try:
            user_input = input("\nç”¨æˆ·: ").strip()
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if not user_input:
                continue

            result = run_chat(user_input, planner_llm, model_name)

            if result["success"]:
                print(f"Agent: {result['response']}")
            else:
                print(f"âŒ é”™è¯¯: {result['error']}")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ æ„å¤–é”™è¯¯: {e}")


if __name__ == "__main__":
    main()